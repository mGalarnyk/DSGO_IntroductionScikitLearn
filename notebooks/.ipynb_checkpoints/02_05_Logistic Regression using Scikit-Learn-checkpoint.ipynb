{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you create a logistic regression model using Scikit-Learn? The first thing you need to know is that despite the name logistic regression containing the word regression, logistic regression is a model used for classification. Classification models can be used for tasks like classifying flower species or image recognition. All of this of course depends on the availability and quality of your data. Logistic Regression has some advantages including\n",
    "\n",
    "* Model training and predictions are relatively fast\n",
    "* No tuning is usually needed for logistic regression unless you want to regularize your model. \n",
    "* Finally, it can perform well with a small number of observations. \n",
    "\n",
    "In this video, I'll share with you how you can create a logistic regession model for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load the Dataset\n",
    " The code below loads a modified version of the iris dataset which has two classes. A 1 is a virginica flower and a 0 is versicolor flower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/modifiedIris2Classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[['petal length (cm)']], df['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Data\n",
    "Logistic Regression is effected by scale so you need to scale the features in the data before using Logistic Regresison. You can transform the data onto unit scale (mean = 0 and variance = 1) for better performance. Scikit-Learn's `StandardScaler` helps standardize the dataset’s features. Note you fit on the training set and transform on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "<b>Step 1:</b> Import the model you want to use\n",
    "\n",
    "In sklearn, all machine learning models are implemented as Python classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was already imported earlier in the notebook so commenting out\n",
    "#from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2:</b> Make an instance of the Model\n",
    "\n",
    "This is a place where we can tune the hyperparameters of a model. Typically this is where you tune C is related to regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3:</b> Training the model on the data, storing the information learned from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is learning the relationship between x (features sepal width, sepal height etc) and y (labels-which species of iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 4:</b> Predict the labels of new data (new flowers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression also allows you to see prediction probabilities as well as  a prediction. This is not like other algorithms like decision trees for classification which only give you a prediction not a probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One observation's petal length after standardization\n",
    "X_test[0].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('prediction', clf.predict(X_test[0].reshape(1,-1))[0])\n",
    "print('probability', clf.predict_proba(X_test[0].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is unclear, let's visualize how logistic regression makes predictions by looking at our test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df = pd.DataFrame()\n",
    "example_df.loc[:, 'petal length (cm)'] = X_test.reshape(-1)\n",
    "example_df.loc[:, 'target'] = y_test.values\n",
    "example_df['logistic_preds'] = pd.DataFrame(clf.predict_proba(X_test))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (10,7));\n",
    "\n",
    "\n",
    "virginicaFilter = example_df['target'] == 1\n",
    "versicolorFilter = example_df['target'] == 0\n",
    "\n",
    "ax.scatter(example_df.loc[virginicaFilter, 'petal length (cm)'].values,\n",
    "            example_df.loc[virginicaFilter, 'logistic_preds'].values,\n",
    "           color = 'g',\n",
    "           s = 60,\n",
    "           label = 'virginica')\n",
    "\n",
    "\n",
    "ax.scatter(example_df.loc[versicolorFilter, 'petal length (cm)'].values,\n",
    "            example_df.loc[versicolorFilter, 'logistic_preds'].values,\n",
    "           color = 'b',\n",
    "           s = 60,\n",
    "           label = 'versicolor')\n",
    "\n",
    "ax.axhline(y = .5, c = 'y')\n",
    "\n",
    "ax.axhspan(.5, 1, alpha=0.05, color='green')\n",
    "ax.axhspan(0, .4999, alpha=0.05, color='blue')\n",
    "ax.text(0.5, .6, 'Classified as viginica', fontsize = 16)\n",
    "ax.text(0.5, .4, 'Classified as versicolor', fontsize = 16)\n",
    "\n",
    "ax.set_ylim(0,1)\n",
    "ax.legend(loc = 'lower right', markerscale = 1.0, fontsize = 12)\n",
    "ax.tick_params(labelsize = 18)\n",
    "ax.set_xlabel('petal length (cm)', fontsize = 24)\n",
    "ax.set_ylabel('probability of virginica', fontsize = 24)\n",
    "ax.set_title('Logistic Regression Predictions', fontsize = 24)\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are other ways of measuring model performance (precision, recall, F1 Score, ROC Curve, etc), let's keep this simple and use accuracy as our metric. \n",
    "To do this are going to see how the model performs on new data (test set)\n",
    "\n",
    "Accuracy is defined as:\n",
    "(fraction of correct predictions): correct predictions / total number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is one metric, but it doesn't say give much insight into what was wrong. Let's look at a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, clf.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True,\n",
    "            fmt=\".0f\",\n",
    "            linewidths=.5,\n",
    "            square = True,\n",
    "            cmap = 'Blues');\n",
    "plt.ylabel('Actual label', fontsize = 17);\n",
    "plt.xlabel('Predicted label', fontsize = 17);\n",
    "plt.title('Accuracy Score: {}'.format(score), size = 17);\n",
    "plt.tick_params(labelsize= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the same information in a table in a clearer way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore this code\n",
    "\n",
    "modified_cm = []\n",
    "for index,value in enumerate(cm):\n",
    "    if index == 0:\n",
    "        modified_cm.append(['TN = ' + str(value[0]), 'FP = ' + str(value[1])])\n",
    "    if index == 1:\n",
    "        modified_cm.append(['FN = ' + str(value[0]), 'TP = ' + str(value[1])])   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=np.array(modified_cm),\n",
    "            fmt=\"\",\n",
    "            annot_kws={\"size\": 20},\n",
    "            linewidths=.5,\n",
    "            square = True,\n",
    "            cmap = 'Blues',\n",
    "            xticklabels = ['versicolor', 'viginica'],\n",
    "            yticklabels = ['versicolor', 'viginica'],\n",
    "            );\n",
    "\n",
    "plt.ylabel('Actual label', fontsize = 17);\n",
    "plt.xlabel('Predicted label', fontsize = 17);\n",
    "plt.title('Accuracy Score: {:.3f}'.format(score), size = 17);\n",
    "plt.tick_params(labelsize= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it, I encourage you to try and create a logistic regression model of your own. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
